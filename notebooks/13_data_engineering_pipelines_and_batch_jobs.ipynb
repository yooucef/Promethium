{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Engineering Pipelines and Batch Jobs\n",
    "\n",
    "Scaling Promethium for production data processing.\n",
    "\n",
    "**Prerequisites:** Experience with notebooks 01-05\n",
    "\n",
    "**Topics:** Batch processing, parallel execution, result management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install promethium-seismic==1.0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import promethium\n",
    "from promethium import (\n",
    "    generate_synthetic_traces,\n",
    "    add_noise,\n",
    "    bandpass_filter,\n",
    "    evaluate_reconstruction,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "\n",
    "set_seed(42)\n",
    "print(f\"Promethium {promethium.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Batch Processing Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_dataset(data, config):\n",
    "    \"\"\"Process a single dataset with given configuration.\"\"\"\n",
    "    # Preprocessing\n",
    "    processed = np.array([\n",
    "        bandpass_filter(t, config['lowcut'], config['highcut'], config['fs'])\n",
    "        for t in data\n",
    "    ])\n",
    "    \n",
    "    # Recovery (simplified)\n",
    "    from scipy.ndimage import gaussian_filter1d\n",
    "    result = np.array([gaussian_filter1d(t, sigma=config.get('sigma', 2)) for t in processed])\n",
    "    \n",
    "    return result\n",
    "\n",
    "def batch_process(datasets, config, output_dir):\n",
    "    \"\"\"Process multiple datasets.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    results = []\n",
    "    \n",
    "    for i, data in enumerate(tqdm(datasets, desc=\"Processing\")):\n",
    "        result = process_single_dataset(data, config)\n",
    "        \n",
    "        # Save result\n",
    "        output_path = os.path.join(output_dir, f'result_{i:04d}.npy')\n",
    "        np.save(output_path, result)\n",
    "        \n",
    "        results.append({\n",
    "            'index': i,\n",
    "            'input_shape': data.shape,\n",
    "            'output_path': output_path\n",
    "        })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multiple test datasets\n",
    "n_datasets = 5\n",
    "datasets = []\n",
    "\n",
    "for i in range(n_datasets):\n",
    "    clean, _ = generate_synthetic_traces(n_traces=50, n_samples=256, seed=42+i)\n",
    "    noisy = add_noise(clean, noise_level=0.3, seed=42+i)\n",
    "    datasets.append(noisy)\n",
    "\n",
    "print(f\"Created {len(datasets)} datasets\")\n",
    "print(f\"Each dataset: {datasets[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'lowcut': 2.0,\n",
    "    'highcut': 80.0,\n",
    "    'fs': 250.0,\n",
    "    'sigma': 2.0\n",
    "}\n",
    "\n",
    "output_dir = './batch_output'\n",
    "results = batch_process(datasets, config, output_dir)\n",
    "\n",
    "print(f\"\\nProcessed {len(results)} datasets\")\n",
    "for r in results:\n",
    "    print(f\"  {r['output_path']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Result Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processing manifest\n",
    "manifest = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'config': config,\n",
    "    'n_datasets': len(results),\n",
    "    'outputs': results\n",
    "}\n",
    "\n",
    "manifest_path = os.path.join(output_dir, 'manifest.json')\n",
    "with open(manifest_path, 'w') as f:\n",
    "    json.dump(manifest, f, indent=2)\n",
    "\n",
    "print(f\"Manifest saved to: {manifest_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and aggregate results\n",
    "all_results = []\n",
    "for r in results:\n",
    "    data = np.load(r['output_path'])\n",
    "    all_results.append(data)\n",
    "\n",
    "stacked = np.stack(all_results)\n",
    "print(f\"Aggregated shape: {stacked.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Parallel Processing Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Parallel processing works best for independent datasets\n",
    "print(\"Parallel Processing Pattern:\")\n",
    "print(\"\"\"\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "def process_one(args):\n",
    "    idx, data, config = args\n",
    "    result = process_single_dataset(data, config)\n",
    "    return idx, result\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=4) as executor:\n",
    "    futures = [executor.submit(process_one, (i, d, config)) \n",
    "               for i, d in enumerate(datasets)]\n",
    "    \n",
    "    for future in as_completed(futures):\n",
    "        idx, result = future.result()\n",
    "        np.save(f'output_{idx}.npy', result)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook covered:\n",
    "1. Batch processing framework\n",
    "2. Result persistence\n",
    "3. Manifest management\n",
    "4. Parallel processing patterns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}